# Federated Learning Configuration â€” FedAdam
# Usage: python -m tpd_fl.fl.server --config configs/fl/fedadam.yaml

# Server
num_rounds: 10
min_clients: 3
strategy: "fedadam"

# FedAdam specific
server_lr: 0.01
beta1: 0.9
beta2: 0.999
tau: 0.001

# Model
model_backend: "synthetic"
vocab_size: 32000
mask_token_id: 0

# LoRA
lora_rank: 8
lora_alpha: 16.0
lora_dropout: 0.0

# Client training
local_epochs: 3
learning_rate: 0.001
batch_size: 4
typed_training: true

# Data
num_clients: 5
domain_skew: 0.8
seed: 42

# TPD
projection_enabled: true
verifier_enabled: true

# Output
output_dir: "runs/fl_fedadam"
